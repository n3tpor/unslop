2025-09-06 12:08:31,871 - solace_ai_connector.common.log_init - INFO - Logging configured via INI file 'configs/logging_config.ini' (specified by LOGGING_CONFIG_PATH) from solace_ai_connector.common.log module import.
2025-09-06 12:08:31,916 - a2a.utils.telemetry - DEBUG - Start tracing for a2a.utils.helpers.create_task_obj, is_async_func False
2025-09-06 12:08:31,916 - a2a.utils.telemetry - DEBUG - Start tracing for a2a.utils.helpers.append_artifact_to_task, is_async_func False
2025-09-06 12:08:32,383 - py.warnings - WARNING - /Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
  warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)

2025-09-06 12:08:35,393 - solace_ai_connector.common.log_init - INFO - Logging configured via INI file 'configs/logging_config.ini' (specified by LOGGING_CONFIG_PATH) from solace_ai_connector.common.log module import.
2025-09-06 12:08:35,439 - a2a.utils.telemetry - DEBUG - Start tracing for a2a.utils.helpers.create_task_obj, is_async_func False
2025-09-06 12:08:35,439 - a2a.utils.telemetry - DEBUG - Start tracing for a2a.utils.helpers.append_artifact_to_task, is_async_func False
2025-09-06 12:08:35,904 - py.warnings - WARNING - /Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
  warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)

2025-09-06 12:08:35,994 - config_portal.backend.server - INFO - Starting Flask app on 127.0.0.1:5002
2025-09-06 12:09:00,561 - LiteLLM - DEBUG - 

2025-09-06 12:09:00,562 - LiteLLM - DEBUG - [92mRequest to litellm:[0m
2025-09-06 12:09:00,562 - LiteLLM - DEBUG - [92mlitellm.completion(model='openai/gemini-2.0-flash-001', api_key='AIzaSyBuulQBVVvXBv-ZPcjJp9LWTIhyNxtqaK4', base_url='https://generativelanguage.googleapis.com/v1beta/openai', messages=[{'role': 'user', 'content': 'Say OK'}])[0m
2025-09-06 12:09:00,563 - LiteLLM - DEBUG - 

2025-09-06 12:09:00,563 - LiteLLM - DEBUG - self.optional_params: {}
2025-09-06 12:09:00,564 - LiteLLM - DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2025-09-06 12:09:00,589 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.0-flash-001; provider = openai
2025-09-06 12:09:00,589 - LiteLLM - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-001', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Say OK'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None}
2025-09-06 12:09:00,589 - LiteLLM - DEBUG - 
LiteLLM: Non-Default params passed to completion() {}
2025-09-06 12:09:00,589 - LiteLLM - DEBUG - Final returned optional params: {'extra_body': {}}
2025-09-06 12:09:00,589 - LiteLLM - DEBUG - self.optional_params: {'extra_body': {}}
2025-09-06 12:09:00,590 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 12:09:00,590 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 12:09:00,590 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 12:09:00,605 - LiteLLM - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/openai/ \
-d '{'model': 'gemini-2.0-flash-001', 'messages': [{'role': 'user', 'content': 'Say OK'}], 'extra_body': {}}'
[0m

2025-09-06 12:09:01,188 - LiteLLM - DEBUG - RAW RESPONSE:
{"id": "nVy8aLSTGe-k_uMP-6X6eA", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "OK\n", "refusal": null, "role": "assistant", "annotations": null, "audio": null, "function_call": null, "tool_calls": null}}], "created": 1757174941, "model": "gemini-2.0-flash-001", "object": "chat.completion", "service_tier": null, "system_fingerprint": null, "usage": {"completion_tokens": 2, "prompt_tokens": 2, "total_tokens": 4, "completion_tokens_details": null, "prompt_tokens_details": null}}


2025-09-06 12:09:01,189 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-06 12:09:01,189 - LiteLLM - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-09-06 12:09:01,190 - LiteLLM - DEBUG - selected model name for cost calculation: openai/gemini-2.0-flash-001
2025-09-06 12:09:01,190 - LiteLLM - DEBUG - selected model name for cost calculation: openai/gemini-2.0-flash-001
2025-09-06 12:09:01,190 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 12:09:01,190 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 12:09:01,190 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - selected model name for cost calculation: openai/gemini-2.0-flash-001
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - selected model name for cost calculation: gemini-2.0-flash-001
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 12:09:01,191 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 12:09:01,201 - LiteLLM - DEBUG - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/utils.py", line 4809, in _get_model_info_helper\n    raise ValueError(\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1228, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 1037, in response_cost_calculator\n    raise e\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 1021, in response_cost_calculator\n    response_cost = completion_cost(\n                    ^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 935, in completion_cost\n    raise e\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 928, in completion_cost\n    raise e\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 890, in completion_cost\n    ) = cost_per_token(\n        ^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 330, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 233, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/utils.py", line 5020, in get_model_info\n    _model_info = _get_model_info_helper(\n                  ^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/utils.py", line 4938, in _get_model_info_helper\n    raise Exception(\nException: This model isn\'t mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'openai/gemini-2.0-flash-001', 'cache_hit': False, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
2025-09-06 12:09:01,202 - LiteLLM - DEBUG - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/utils.py", line 4809, in _get_model_info_helper\n    raise ValueError(\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1228, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 1037, in response_cost_calculator\n    raise e\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 1021, in response_cost_calculator\n    response_cost = completion_cost(\n                    ^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 935, in completion_cost\n    raise e\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 928, in completion_cost\n    raise e\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 890, in completion_cost\n    ) = cost_per_token(\n        ^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 330, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 233, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/utils.py", line 5020, in get_model_info\n    _model_info = _get_model_info_helper(\n                  ^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/utils.py", line 4938, in _get_model_info_helper\n    raise Exception(\nException: This model isn\'t mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'gemini-2.0-flash-001', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
2025-09-06 12:09:01,203 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 12:09:01,203 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 12:09:01,203 - LiteLLM - DEBUG - Model=gemini-2.0-flash-001 is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - selected model name for cost calculation: openai/gemini-2.0-flash-001
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - selected model name for cost calculation: gemini-2.0-flash-001
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/utils.py", line 4809, in _get_model_info_helper\n    raise ValueError(\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1228, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 1037, in response_cost_calculator\n    raise e\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 1021, in response_cost_calculator\n    response_cost = completion_cost(\n                    ^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 935, in completion_cost\n    raise e\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 928, in completion_cost\n    raise e\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 890, in completion_cost\n    ) = cost_per_token(\n        ^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/cost_calculator.py", line 330, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 233, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/utils.py", line 5020, in get_model_info\n    _model_info = _get_model_info_helper(\n                  ^^^^^^^^^^^^^^^^^^^^^^^\n  File "/Users/thomas/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/utils.py", line 4938, in _get_model_info_helper\n    raise Exception(\nException: This model isn\'t mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'gemini-2.0-flash-001', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 12:09:01,204 - LiteLLM - DEBUG - Model=gemini-2.0-flash-001 is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
